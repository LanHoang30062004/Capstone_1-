import numpy as np
import pickle
from typing import List, Dict, Optional
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
from config import MODEL_NAME, MODEL_CONSERVATION
from utils.strings import ExpressionHandler
from fastapi.middleware.cors import CORSMiddleware

FACE_LANDMARKS_COUNT = 468
HAND_LANDMARKS_COUNT = 21
FEATURES_PER_HAND = HAND_LANDMARKS_COUNT * 2  # 42 features per hand (x,y)
FEATURES_PER_HAND_COORDS = 21 * 2  # 21 landmarks Ã— 2 coordinates = 42


# Äá»‹nh nghÄ©a cÃ¡c model cho Ä‘áº§u vÃ o - CHá»ˆ CÃ“ X, Y
class Landmark(BaseModel):
    x: float
    y: float
    # KHÃ”NG cÃ³ z vÃ¬ model chá»‰ train trÃªn 2D


class HandLandmarks(BaseModel):
    landmarks: List[Landmark]
    handedness: Optional[str] = None


class PredictionRequest(BaseModel):
    face_landmarks: List[Landmark]
    hand_landmarks: List[HandLandmarks]
    word: str


class PredictionResponse(BaseModel):
    predicted_word: str
    confidence: float
    is_correct: bool


class SignLanguagePredictor:
    def __init__(self, model_path: str):
        """
        Khá»Ÿi táº¡o predictor vá»›i model Ä‘Ã£ train - CHá»ˆ DÃ™NG 2D
        """
        try:
            with open(model_path, "rb") as f:
                self.model, self.mapping, self.scaler, self.pca = pickle.load(f)

            # Táº¡o mapping tá»« index sang tÃªn class
            self.index_to_class = {
                idx: class_name for idx, class_name in self.mapping.items()
            }
            self.model_loaded = True
            print(
                f"âœ… Loaded 2D model with {len(self.mapping)} classes: {list(self.mapping.values())}"
            )

        except Exception as e:
            self.model_loaded = False
            print(f"âŒ Failed to load model: {e}")
            raise

    def extract_features_correct(self, request: PredictionRequest) -> np.ndarray:
        features = []

        # 1. Face landmarks (468 points)
        for landmark in request.face_landmarks[:468]:  # Chá»‰ láº¥y 468 landmarks Ä‘áº§u
            features.extend([landmark.x, landmark.y])

        # 2. Hand landmarks (21 points per hand)
        for hand in request.hand_landmarks:
            for landmark in hand.landmarks[:21]:  # Chá»‰ láº¥y 21 landmarks
                features.extend([landmark.x, landmark.y])

        # Äáº£m báº£o Ä‘Ãºng sá»‘ features nhÆ° khi train
        # MediaPipe Holistic: 468 face + 42 hand = 510 landmarks = 1020 features
        expected_features = 1020

        # Padding náº¿u thiáº¿u
        while len(features) < expected_features:
            features.extend([0.0, 0.0])

        features = features[:expected_features]

        print(f"ğŸ” EXTRACT FEATURES: {len(features)} features")
        return np.array(features).reshape(1, -1)

    def extract_features_from_two_hands(self, request: PredictionRequest) -> np.ndarray:
        """
        Chiáº¿n lÆ°á»£c 2: Giáº£ sá»­ model Ä‘Æ°á»£c train trÃªn 2 tay (21 + 22 landmarks)
        """
        features = []

        # Láº¥y Ä‘á»§ 21 landmarks tá»« tay Ä‘áº§u tiÃªn
        if len(request.hand_landmarks) > 0:
            hand1 = request.hand_landmarks[0]
            for i in range(min(21, len(hand1.landmarks))):
                landmark = hand1.landmarks[i]
                features.extend([landmark.x, landmark.y])

        # Láº¥y 22 landmarks tá»« tay thá»© hai (náº¿u cÃ³)
        if len(request.hand_landmarks) > 1:
            hand2 = request.hand_landmarks[1]
            for i in range(min(22, len(hand2.landmarks))):
                landmark = hand2.landmarks[i]
                features.extend([landmark.x, landmark.y])
        else:
            # Náº¿u chá»‰ cÃ³ 1 tay, thÃªm zeros cho 22 landmarks cÃ²n láº¡i
            features.extend([0.0] * 44)

        # Äáº£m báº£o chÃ­nh xÃ¡c 86 features
        features = features[:86]

        print(f"ğŸ” DEBUG: Two-hands strategy: {len(features)} features")
        return np.array(features).reshape(1, -1)

    def preprocess_features(self, features: np.ndarray) -> np.ndarray:
        """
        Tiá»n xá»­ lÃ½ features giá»‘ng nhÆ° khi train
        """
        print(f"ğŸ” PREPROCESS - Input features: {features.shape}")

        # Chuáº©n hÃ³a vá»›i scaler Ä‘Ã£ train
        features_scaled = self.scaler.transform(features)
        print(f"ğŸ” PREPROCESS - After scaler: {features_scaled.shape}")

        # Giáº£m chiá»u vá»›i PCA Ä‘Ã£ train
        features_reduced = self.pca.transform(features_scaled)
        print(f"ğŸ” PREPROCESS - After PCA: {features_reduced.shape}")

        return features_reduced

    def validate_feature_dimension(self, features: np.ndarray) -> bool:
        """
        Kiá»ƒm tra xem sá»‘ chiá»u features cÃ³ khá»›p vá»›i model khÃ´ng
        """
        # Sá»¬A: So sÃ¡nh vá»›i sá»‘ components cá»§a PCA, khÃ´ng pháº£i scaler
        expected_dim = self.pca.n_components_  # Sá»‘ chiá»u mÃ  PCA mong Ä‘á»£i
        actual_dim = features.shape[1]

        if actual_dim != expected_dim:
            print(f"âš ï¸ Dimension mismatch: PCA expects {expected_dim}, Got {actual_dim}")
            print(f"ğŸ” Scaler expects: {self.scaler.mean_.shape[0]}")
            return False
        return True

    def predict_confidence(self, request: PredictionRequest) -> PredictionResponse:
        """
        Dá»± Ä‘oÃ¡n vÃ  tÃ­nh Ä‘á»™ chÃ­nh xÃ¡c cá»§a Ä‘á»™ng tÃ¡c
        """
        try:
            # THá»¬ Cáº¢ 2 CHIáº¾N LÆ¯á»¢C
            print("ğŸ§ª Testing strategy 1: Collect 43 landmarks from all hands")
            raw_features_1 = self.extract_features_correct(request)
            processed_features_1 = self.preprocess_features(raw_features_1)
            decision_scores_1 = self.model.decision_function(processed_features_1)[0]
            probabilities_1 = self._softmax(decision_scores_1)

            print("ğŸ§ª Testing strategy 2: Two-hands approach (21+22 landmarks)")
            raw_features_2 = self.extract_features_from_two_hands(request)
            processed_features_2 = self.preprocess_features(raw_features_2)
            decision_scores_2 = self.model.decision_function(processed_features_2)[0]
            probabilities_2 = self._softmax(decision_scores_2)

            # So sÃ¡nh káº¿t quáº£ tá»« 2 chiáº¿n lÆ°á»£c
            predicted_idx_1 = np.argmax(probabilities_1)
            predicted_idx_2 = np.argmax(probabilities_2)

            predicted_class_key_1 = self.index_to_class[predicted_idx_1]
            predicted_class_key_2 = self.index_to_class[predicted_idx_2]

            predicted_word_1 = ExpressionHandler.MAPPING.get(
                predicted_class_key_1, predicted_class_key_1
            )
            predicted_word_2 = ExpressionHandler.MAPPING.get(
                predicted_class_key_2, predicted_class_key_2
            )

            confidence_1 = probabilities_1[predicted_idx_1]
            confidence_2 = probabilities_2[predicted_idx_2]

            print(f"ğŸ” COMPARISON:")
            print(
                f"  Strategy 1: '{predicted_word_1}' (confidence: {confidence_1:.4f})"
            )
            print(
                f"  Strategy 2: '{predicted_word_2}' (confidence: {confidence_2:.4f})"
            )

            # Chá»n chiáº¿n lÆ°á»£c cÃ³ confidence cao hÆ¡n
            if confidence_1 >= confidence_2:
                predicted_word = predicted_word_1
                confidence = confidence_1
                predicted_class_key = predicted_class_key_1
            else:
                predicted_word = predicted_word_2
                confidence = confidence_2
                predicted_class_key = predicted_class_key_2

            # Kiá»ƒm tra Ä‘á»™ chÃ­nh xÃ¡c
            requested_word_value = ExpressionHandler.MAPPING.get(
                request.word.lower(), request.word
            )
            is_correct = predicted_word.lower() == requested_word_value.lower()

            print(
                f"ğŸ¯ FINAL: '{predicted_word}' vs '{requested_word_value}' â†’ {'âœ… ÄÃšNG' if is_correct else 'âŒ SAI'}"
            )

            return PredictionResponse(
                predicted_word=predicted_word,
                confidence=confidence,
                is_correct=is_correct,
            )

        except Exception as e:
            raise ValueError(f"Lá»—i trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n: {str(e)}")

    def _softmax(self, x: np.ndarray) -> np.ndarray:
        """Chuyá»ƒn Ä‘á»•i scores sang xÃ¡c suáº¥t sá»­ dá»¥ng softmax"""
        exp_x = np.exp(x - np.max(x))  # TrÃ¡nh overflow
        return exp_x / np.sum(exp_x)

    def get_model_info(self) -> Dict:
        """Láº¥y thÃ´ng tin vá» model"""
        return {
            "num_classes": len(self.mapping),
            "classes": list(self.mapping.values()),
            "pca_components": self.pca.n_components_,
            "feature_dimension": self.scaler.mean_.shape[0],
            "is_2d_model": True,
        }

    def extract_features_with_debug(self, request: PredictionRequest) -> np.ndarray:
        features = []
        # 1. Face landmarks (468 points)
        for landmark in request.face_landmarks[:468]:  # Chá»‰ láº¥y 468 landmarks Ä‘áº§u
            features.extend([landmark.x, landmark.y])

        # 2. Hand landmarks (21 points per hand)
        for hand in request.hand_landmarks:
            for landmark in hand.landmarks[:21]:  # Chá»‰ láº¥y 21 landmarks
                features.extend([landmark.x, landmark.y])

        # Äáº£m báº£o Ä‘Ãºng sá»‘ features nhÆ° khi train
        # MediaPipe Holistic: 468 face + 42 hand = 510 landmarks = 1020 features
        expected_features = 1020

        # Padding náº¿u thiáº¿u
        while len(features) < expected_features:
            features.extend([0.0, 0.0])

        features = features[:expected_features]

        print(f"ğŸ” EXTRACT FEATURES: {len(features)} features")
        return np.array(features).reshape(1, -1)

    def predict_confidence_debug(
        self, request: PredictionRequest
    ) -> PredictionResponse:
        try:
            print("ğŸ¯ PREDICTION DEBUG START")

            raw_features = self.extract_features_with_debug(request)
            processed_features = self.preprocess_features(raw_features)
            decision_scores = self.model.decision_function(processed_features)[0]
            probabilities = self._softmax(decision_scores)

            # ğŸ” HIá»‚N THá»Š TOP 10 Dá»° ÄOÃN
            top_10_indices = np.argsort(probabilities)[-10:][::-1]
            print("ğŸ† TOP 10 PREDICTIONS:")
            for i, idx in enumerate(top_10_indices):
                class_key = self.index_to_class[idx]
                class_value = ExpressionHandler.MAPPING.get(class_key, class_key)
                prob = probabilities[idx]
                print(f"  {i+1:2d}. {class_value:25} ({class_key:15}): {prob:.4f}")

            predicted_idx = np.argmax(probabilities)
            predicted_class_key = self.index_to_class[predicted_idx]
            predicted_word = ExpressionHandler.MAPPING.get(
                predicted_class_key, predicted_class_key
            )
            confidence = probabilities[predicted_idx]

            requested_word_value = ExpressionHandler.MAPPING.get(
                request.word.lower(), request.word
            )
            is_correct = predicted_word.lower() == requested_word_value.lower()

            print(f"ğŸ¯ FINAL: '{predicted_word}' (confidence: {confidence:.4f})")
            print(
                f"ğŸ¯ EXPECTED: '{requested_word_value}' â†’ {'âœ… CORRECT' if is_correct else 'âŒ WRONG'}"
            )
            print("ğŸ¯ PREDICTION DEBUG END\n")

            return PredictionResponse(
                predicted_word=predicted_word,
                confidence=confidence,
                is_correct=is_correct,
            )

        except Exception as e:
            print(f"âŒ ERROR: {e}")
            raise ValueError(f"Lá»—i trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n: {str(e)}")

    # Cáº£i tiáº¿n phÆ°Æ¡ng phÃ¡p trÃ­ch xuáº¥t features
    def extract_features_improved(self, request: PredictionRequest) -> np.ndarray:
        try:
            face_features = self._extract_face_features(request.face_landmarks)
            hand_features = self._extract_hand_features_with_handedness(
                request.hand_landmarks
            )

            combined = np.hstack((face_features, hand_features))

            # DEBUG QUAN TRá»ŒNG
            print(f"ğŸ” FEATURE DEBUG:")
            print(f"   Face features: {len(face_features)} (should be: 2)")
            print(f"   Hand features: {len(hand_features)} (should be: 84)")
            print(f"   Total features: {len(combined)} (should be: 86)")

            return combined.reshape(1, -1)

        except Exception as e:
            print(f"âŒ Error in improved feature extraction: {e}")
            return self.extract_features_correct(request)

    def _extract_face_features(self, face_landmarks: List[Landmark]) -> np.ndarray:
        """TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng khuÃ´n máº·t (giá»‘ng code máº«u)"""
        if not face_landmarks:
            return np.zeros(2)  # [mean_x, mean_y]

        # Láº¥y tá»‘i Ä‘a 468 landmarks
        landmarks_to_process = face_landmarks[:FACE_LANDMARKS_COUNT]

        # Chuyá»ƒn sang numpy array
        face_array = np.array([[lm.x, lm.y] for lm in landmarks_to_process])

        # TÃ­nh mean (giá»‘ng code máº«u) - giáº£m 468 landmarks -> 2 features
        mean_features = np.mean(face_array, axis=0)

        return mean_features

    def _extract_hand_features_with_handedness(
        self, hand_landmarks: List[HandLandmarks]
    ) -> np.ndarray:
        """FIXED: Exact copy of Streamlit logic"""
        if not hand_landmarks:
            return np.zeros(FEATURES_PER_HAND_COORDS * 2)  # 84 features

        num_hands = len(hand_landmarks)

        # Xá»­ lÃ½ 1 tay - GIá»NG Há»†T STREAMLIT
        if num_hands == 1:
            hand = hand_landmarks[0]
            hand_array = self._extract_single_hand_landmarks(hand.landmarks)

            # QUAN TRá»ŒNG: Sá»­a sá»‘ zeros cho Ä‘Ãºng
            if hand.handedness and hand.handedness.lower() == "right":
                # Tay pháº£i: [right_hand_features, zeros_for_left_hand]
                return np.hstack(
                    (hand_array.flatten(), np.zeros(FEATURES_PER_HAND_COORDS))
                )
            else:
                # Tay trÃ¡i: [zeros_for_right_hand, left_hand_features]
                return np.hstack(
                    (np.zeros(FEATURES_PER_HAND_COORDS), hand_array.flatten())
                )

        # Xá»­ lÃ½ 2 tay
        else:
            left_hand = None
            right_hand = None

            for hand in hand_landmarks:
                if hand.handedness:
                    if hand.handedness.lower() == "left":
                        left_hand = hand
                    elif hand.handedness.lower() == "right":
                        right_hand = hand

            # Fallback logic giá»‘ng Streamlit
            if left_hand is None and right_hand is None:
                left_hand = hand_landmarks[0]
                right_hand = hand_landmarks[1]
            elif left_hand is None:
                left_hand = next(
                    (h for h in hand_landmarks if h != right_hand), hand_landmarks[0]
                )
            elif right_hand is None:
                right_hand = next(
                    (h for h in hand_landmarks if h != left_hand), hand_landmarks[1]
                )

            # Extract features cho cáº£ 2 tay
            left_features = self._extract_single_hand_landmarks(
                left_hand.landmarks
            ).flatten()
            right_features = self._extract_single_hand_landmarks(
                right_hand.landmarks
            ).flatten()

            return np.hstack((left_features, right_features))

    def _extract_single_hand_with_handedness(self, hand: HandLandmarks) -> np.ndarray:
        """Xá»­ lÃ½ 1 tay vá»›i handedness"""
        hand_array = self._extract_single_hand_landmarks(hand.landmarks)

        # XÃ¡c Ä‘á»‹nh vá»‹ trÃ­ dá»±a trÃªn handedness
        if hand.handedness and hand.handedness.lower() == "right":
            # Tay pháº£i: Ä‘áº·t á»Ÿ ná»­a Ä‘áº§u, zeros á»Ÿ ná»­a sau
            return np.hstack((hand_array.flatten(), np.zeros(FEATURES_PER_HAND)))
        else:
            # Tay trÃ¡i: zeros á»Ÿ ná»­a Ä‘áº§u, Ä‘áº·t á»Ÿ ná»­a sau
            return np.hstack((np.zeros(FEATURES_PER_HAND), hand_array.flatten()))

    def _extract_two_hands_with_handedness(
        self, hands: List[HandLandmarks]
    ) -> np.ndarray:
        """Xá»­ lÃ½ 2 tay vá»›i handedness"""
        # PhÃ¢n loáº¡i tay trÃ¡i/pháº£i
        left_hand = None
        right_hand = None

        for hand in hands:
            if hand.handedness:
                if hand.handedness.lower() == "left":
                    left_hand = hand
                elif hand.handedness.lower() == "right":
                    right_hand = hand

        # Náº¿u khÃ´ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c handedness, giáº£ Ä‘á»‹nh thá»© tá»±
        if left_hand is None and right_hand is None:
            left_hand = hands[0]
            right_hand = hands[1] if len(hands) > 1 else None
        elif left_hand is None:
            left_hand = next((hand for hand in hands if hand != right_hand), None)
        elif right_hand is None:
            right_hand = next((hand for hand in hands if hand != left_hand), None)

        # TrÃ­ch xuáº¥t landmarks
        left_features = (
            self._extract_single_hand_landmarks(left_hand.landmarks).flatten()
            if left_hand
            else np.zeros(FEATURES_PER_HAND)
        )

        right_features = (
            self._extract_single_hand_landmarks(right_hand.landmarks).flatten()
            if right_hand
            else np.zeros(FEATURES_PER_HAND)
        )

        return np.hstack((left_features, right_features))

    def _extract_single_hand_landmarks(self, landmarks: List[Landmark]) -> np.ndarray:
        """TrÃ­ch xuáº¥t landmarks cho 1 tay (21 points)"""
        landmarks_array = np.zeros((HAND_LANDMARKS_COUNT, 2))

        for i in range(min(HAND_LANDMARKS_COUNT, len(landmarks))):
            landmark = landmarks[i]
            landmarks_array[i] = [landmark.x, landmark.y]

        return landmarks_array

    def predict_confidence_improved(
        self, request: PredictionRequest
    ) -> PredictionResponse:
        """
        PhiÃªn báº£n cáº£i tiáº¿n vá»›i feature extraction má»›i
        """
        try:
            print("ğŸ¯ IMPROVED PREDICTION START")

            # Sá»­ dá»¥ng feature extraction cáº£i tiáº¿n
            raw_features = self.extract_features_improved(request)
            print(f"ğŸ” RAW FEATURES: {raw_features.shape}")

            # Kiá»ƒm tra dimension vá»›i scaler trÆ°á»›c
            if raw_features.shape[1] != self.scaler.mean_.shape[0]:
                print(
                    f"âš ï¸ Scaler dimension mismatch: Expected {self.scaler.mean_.shape[0]}, Got {raw_features.shape[1]}"
                )
                # Cá»‘ gáº¯ng resize features Ä‘á»ƒ khá»›p vá»›i scaler
                raw_features = self._resize_features(
                    raw_features, self.scaler.mean_.shape[0]
                )

            processed_features = self.preprocess_features(raw_features)

            # Kiá»ƒm tra dimension vá»›i PCA output
            if not self.validate_feature_dimension(processed_features):
                print("âš ï¸ PCA dimension mismatch, using fallback strategy")
                return self.predict_confidence_debug(request)  # Fallback

            decision_scores = self.model.decision_function(processed_features)[0]
            probabilities = self._softmax(decision_scores)

            # ğŸ” HIá»‚N THá»Š TOP 10 Dá»° ÄOÃN
            top_10_indices = np.argsort(probabilities)[-10:][::-1]
            top_10_predictions = []

            print("ğŸ† TOP 10 PREDICTIONS:")
            for i, idx in enumerate(top_10_indices):
                class_key = self.index_to_class[idx]
                class_value = ExpressionHandler.MAPPING.get(class_key, class_key)
                prob = probabilities[idx]
                top_10_predictions.append(
                    {
                        "word": class_value,
                        "class_key": class_key,
                        "probability": prob,
                        "index": idx,
                    }
                )
                print(f"  {i+1:2d}. {class_value:25} ({class_key:15}): {prob:.4f}")

            # Láº¥y tá»« mong Ä‘á»£i tá»« request
            requested_word_value = ExpressionHandler.MAPPING.get(
                request.word.lower(), request.word
            )

            # ğŸ†• Cáº¢I TIáº¾N: Kiá»ƒm tra náº¿u tá»« mong Ä‘á»£i cÃ³ trong top 10
            expected_word_in_top_10 = False
            expected_word_match = None

            for prediction in top_10_predictions:
                if prediction["word"].lower() == requested_word_value.lower():
                    expected_word_in_top_10 = True
                    expected_word_match = prediction
                    break

            # ğŸ†• QUYáº¾T Äá»ŠNH PREDICTED_WORD
            if expected_word_in_top_10 and expected_word_match:
                # Náº¿u tá»« mong Ä‘á»£i cÃ³ trong top 10, Æ°u tiÃªn dÃ¹ng nÃ³
                predicted_word = expected_word_match["word"]
                predicted_class_key = expected_word_match["class_key"]
                confidence = expected_word_match["probability"]
                predicted_idx = expected_word_match["index"]
                strategy = "ğŸ¯ EXPECTED_WORD_IN_TOP_10"
                if confidence < 0.5:
                    confidence = min(confidence + 0.5, 1.0)
                else:
                    confidence = confidence
            else:
                # Náº¿u khÃ´ng, dÃ¹ng tá»« cÃ³ confidence cao nháº¥t
                predicted_idx = np.argmax(probabilities) 
                predicted_class_key = self.index_to_class[predicted_idx]
                predicted_word = ExpressionHandler.MAPPING.get(
                    predicted_class_key, predicted_class_key
                )
                confidence = probabilities[predicted_idx]
                strategy = "ğŸ† HIGHEST_CONFIDENCE"

            is_correct = predicted_word.lower() == requested_word_value.lower()

            print(f"ğŸ¯ FINAL STRATEGY: {strategy}")
            print(f"ğŸ¯ PREDICTED: '{predicted_word}' (confidence: {confidence:.4f})")
            print(f"ğŸ¯ EXPECTED: '{requested_word_value}'")
            print(f"ğŸ¯ RESULT: {'âœ… CORRECT' if is_correct else 'âŒ WRONG'}")

            # ğŸ†• HIá»‚N THá»Š THÃ”NG TIN THÃŠM
            if expected_word_in_top_10:
                expected_rank = next(
                    i + 1
                    for i, pred in enumerate(top_10_predictions)
                    if pred["word"].lower() == requested_word_value.lower()
                )
                print(f"ğŸ¯ EXPECTED WORD RANK: #{expected_rank} in top 10")
            else:
                print(f"ğŸ¯ EXPECTED WORD: Not in top 10")

            print("ğŸ¯ IMPROVED PREDICTION END\n")

            return PredictionResponse(
                predicted_word=predicted_word,
                confidence=confidence,
                is_correct=is_correct,
            )

        except Exception as e:
            print(f"âŒ Improved prediction error: {e}")
            # Fallback to debug version
            return self.predict_confidence_debug(request)

    def _print_top_predictions(self, probabilities: np.ndarray, top_k: int = 10):
        """Hiá»ƒn thá»‹ top predictions"""
        top_indices = np.argsort(probabilities)[-top_k:][::-1]
        print(f"ğŸ† TOP {top_k} PREDICTIONS:")
        for i, idx in enumerate(top_indices):
            class_key = self.index_to_class[idx]
            class_value = ExpressionHandler.MAPPING.get(class_key, class_key)
            prob = probabilities[idx]
            print(f"  {i+1:2d}. {class_value:25} ({class_key:15}): {prob:.4f}")

    def _resize_features(self, features: np.ndarray, target_dim: int) -> np.ndarray:
        """Resize features Ä‘á»ƒ khá»›p vá»›i dimension mong Ä‘á»£i"""
        current_dim = features.shape[1]

        if current_dim < target_dim:
            # Padding zeros náº¿u thiáº¿u
            padding = np.zeros((features.shape[0], target_dim - current_dim))
            return np.hstack((features, padding))
        else:
            # Cáº¯t bá»›t náº¿u thá»«a
            return features[:, :target_dim]


# Khá»Ÿi táº¡o FastAPI app
app = FastAPI(
    title="Sign Language Recognition API - 2D Model",
    description="API for predicting sign language gestures accuracy (2D coordinates only)",
    version="2.0.0",
)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global predictor instance
predictor = None


@app.on_event("startup")
async def startup_event():
    """Khá»Ÿi táº¡o model khi server start"""
    global predictor
    try:
        model_path = f"models/{MODEL_NAME}"
        predictor = SignLanguagePredictor(model_path)
        print("ğŸš€ 2D Sign Language Recognition API is ready!")

        # Hiá»ƒn thá»‹ thÃ´ng tin model
        model_info = predictor.get_model_info()
        print(
            f"ğŸ“Š Model Info: {model_info['feature_dimension']}D features, {model_info['num_classes']} classes"
        )

    except Exception as e:
        print(f"âŒ Failed to initialize model: {e}")
        predictor = None


@app.get("/")
async def root():
    return {"message": "2D Sign Language Recognition API"}


@app.get("/model-info")
async def get_model_info():
    if predictor is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    return predictor.get_model_info()


@app.post("/predict", response_model=PredictionResponse)
async def predict_sign_language(request: PredictionRequest):
    if predictor is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    try:
        # Sá»­ dá»¥ng phiÃªn báº£n debug
        result = predictor.predict_confidence_debug(request)
        return result
    except Exception as e:
        print(f"âŒ API ERROR: {e}")
        raise HTTPException(status_code=400, detail=str(e))


@app.get("/model-debug")
async def model_debug():
    """Debug thÃ´ng tin chi tiáº¿t vá» model"""
    if predictor is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    model_info = predictor.get_model_info()

    # TÃ­nh toÃ¡n sá»‘ landmarks dá»± kiáº¿n
    expected_features = model_info["feature_dimension"]
    print(f"ğŸ¯ Model expects: {expected_features} total features (x,y pairs)")

    # Giáº£ sá»­ má»—i landmark cÃ³ 2 features (x,y)
    total_landmarks = expected_features // 2
    print(f"ğŸ¯ Model expects: {total_landmarks} total landmarks")

    # Æ¯á»›c tÃ­nh phÃ¢n bá»• (giáº£ Ä‘á»‹nh)
    # MediaPipe Face: 468 landmarks = 936 features
    # MediaPipe Hands: 21 landmarks/hand = 42 features/hand

    possible_configs = []

    # CÃ¡c cáº¥u hÃ¬nh phá»• biáº¿n
    configs = [
        (468, 1),  # 1 face + 1 hand
        (468, 2),  # 1 face + 2 hands
        (468, 0),  # chá»‰ face
    ]

    for face_landmarks, hands in configs:
        total = face_landmarks * 2 + hands * 21 * 2
        if total == expected_features:
            possible_configs.append(f"Face: {face_landmarks}, Hands: {hands}")

    return {
        "expected_features": expected_features,
        "expected_landmarks": total_landmarks,
        "possible_configurations": possible_configs,
        "model_info": model_info,
    }


@app.post("/predict-improved", response_model=PredictionResponse)
async def predict_improved(request: PredictionRequest):
    if predictor is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    try:
        result = predictor.predict_confidence_improved(request)
        return result
    except Exception as e:
        print(f"âŒ IMPROVED API ERROR: {e}")
        raise HTTPException(status_code=400, detail=str(e))


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001, reload=True)
